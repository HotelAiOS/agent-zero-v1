#!/usr/bin/env python3
"""
Agent Zero V1 - Enhanced Analytics Dashboard Backend
V2.0 Intelligence Layer - Week 44 Implementation

ðŸŽ¯ Week 44 Critical Task: Enhanced Analytics Dashboard Backend (2 SP)
Zadanie: API endpoints dla V2.0 metrics, real-time data streaming
Rezultat: Real-time business insights
Impact: Developer B moÅ¼e zintegrowaÄ‡ advanced analytics w frontend

Author: Developer A (Backend Architect)
Date: 10 paÅºdziernika 2025
Linear Issue: A0-44 (Week 44 Implementation)
"""

import os
import json
import sqlite3
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, AsyncGenerator
from dataclasses import dataclass, asdict
from enum import Enum
import logging

# FastAPI imports with fallback
try:
    from fastapi import FastAPI, WebSocket, HTTPException, BackgroundTasks
    from fastapi.responses import StreamingResponse
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel
    FASTAPI_AVAILABLE = True
except ImportError:
    FASTAPI_AVAILABLE = False
    logging.warning("FastAPI not available. Install with: pip install fastapi uvicorn")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MetricType(Enum):
    SUCCESS_RATE = "success_rate"
    COST_ANALYSIS = "cost_analysis"
    LATENCY_TRENDS = "latency_trends"
    MODEL_PERFORMANCE = "model_performance"
    PATTERN_INSIGHTS = "pattern_insights"
    RECOMMENDATIONS = "recommendations"
    SYSTEM_HEALTH = "system_health"

class TimeRange(Enum):
    HOUR = "1h"
    DAY = "24h"
    WEEK = "7d"
    MONTH = "30d"
    QUARTER = "90d"

@dataclass
class AnalyticsMetric:
    """Single analytics metric with metadata"""
    id: str
    metric_type: MetricType
    name: str
    value: Union[float, int, str, Dict, List]
    unit: str
    trend: Optional[str]  # "up", "down", "stable"
    change_percentage: Optional[float]
    timestamp: datetime
    metadata: Dict[str, Any]

@dataclass
class DashboardData:
    """Complete dashboard data package"""
    timestamp: datetime
    system_status: str
    key_metrics: List[AnalyticsMetric]
    trends: Dict[str, List[Dict]]
    alerts: List[Dict[str, Any]]
    recommendations: List[Dict[str, Any]]
    performance_summary: Dict[str, Any]

class AnalyticsDashboardAPI:
    """
    Enhanced Analytics Dashboard Backend for Agent Zero V2.0
    
    Responsibilities:
    - Provide real-time metrics API endpoints
    - Stream live analytics data via WebSocket
    - Aggregate V2.0 intelligence layer data
    - Generate business insights and KPIs
    - Support Developer B frontend integration
    """
    
    def __init__(self, db_path: str = "agent_zero.db"):
        self.db_path = db_path
        self.active_connections: List[WebSocket] = []
        
        if FASTAPI_AVAILABLE:
            self.app = FastAPI(
                title="Agent Zero V2.0 Analytics API",
                description="Enhanced analytics backend with V2.0 intelligence integration",
                version="2.0.0"
            )
            self._setup_fastapi_routes()
        else:
            self.app = None
            logger.warning("FastAPI not available - running in mock mode")
    
    def _setup_fastapi_routes(self):
        """Setup FastAPI routes for analytics endpoints"""
        
        # CORS middleware
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        @self.app.get("/api/v2/analytics/overview")
        async def get_analytics_overview(time_range: str = "24h"):
            """Get comprehensive analytics overview"""
            return self.get_analytics_overview(TimeRange(time_range))
        
        @self.app.get("/api/v2/analytics/metrics/{metric_type}")
        async def get_specific_metric(metric_type: str, time_range: str = "24h"):
            """Get specific metric data"""
            try:
                metric_enum = MetricType(metric_type)
                return self.get_metric_data(metric_enum, TimeRange(time_range))
            except ValueError:
                raise HTTPException(status_code=400, detail="Invalid metric type")
        
        @self.app.get("/api/v2/analytics/real-time-summary")
        async def get_real_time_summary():
            """Get real-time system summary"""
            return self.get_real_time_summary()
        
        @self.app.get("/api/v2/analytics/cost-optimization")
        async def get_cost_optimization():
            """Get cost optimization insights"""
            return self.get_cost_optimization_data()
        
        @self.app.get("/api/v2/analytics/pattern-insights")
        async def get_pattern_insights():
            """Get pattern mining insights"""
            return self.get_pattern_insights_data()
        
        @self.app.get("/api/v2/analytics/model-performance")
        async def get_model_performance():
            """Get model performance analytics"""
            return self.get_model_performance_data()
        
        @self.app.websocket("/api/v2/analytics/live-stream")
        async def websocket_analytics_stream(websocket: WebSocket):
            """WebSocket endpoint for live analytics streaming"""
            await websocket.accept()
            self.active_connections.append(websocket)
            
            try:
                while True:
                    # Send live analytics data every 5 seconds
                    live_data = self.get_real_time_summary()
                    await websocket.send_json(live_data)
                    await asyncio.sleep(5)
            except Exception as e:
                logger.error(f"WebSocket connection error: {e}")
            finally:
                if websocket in self.active_connections:
                    self.active_connections.remove(websocket)
        
        @self.app.post("/api/v2/analytics/trigger-update")
        async def trigger_analytics_update(background_tasks: BackgroundTasks):
            """Trigger analytics data refresh"""
            background_tasks.add_task(self._refresh_analytics_cache)
            return {"status": "refresh_triggered", "timestamp": datetime.now().isoformat()}
    
    def get_analytics_overview(self, time_range: TimeRange = TimeRange.DAY) -> Dict[str, Any]:
        """Get comprehensive analytics overview for dashboard"""
        
        days_back = self._get_days_from_range(time_range)
        cutoff_date = (datetime.now() - timedelta(days=days_back)).isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            # Core metrics
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total_tasks,
                    AVG(success_score) as avg_success,
                    SUM(cost_usd) as total_cost,
                    AVG(latency_ms) as avg_latency,
                    COUNT(DISTINCT model_used) as models_used,
                    MAX(timestamp) as last_activity
                FROM simple_tracker
                WHERE timestamp >= ?
            """, [cutoff_date])
            
            core_metrics = cursor.fetchone()
            
            # V2.0 enhanced metrics
            v2_metrics = {}
            try:
                cursor = conn.execute("""
                    SELECT 
                        COUNT(*) as v2_evaluations,
                        AVG(overall_score) as v2_avg_success,
                        COUNT(DISTINCT model_used) as v2_models_evaluated
                    FROM v2_success_evaluations
                    WHERE timestamp >= ?
                """, [cutoff_date])
                
                v2_data = cursor.fetchone()
                if v2_data and v2_data[0] > 0:
                    v2_metrics = {
                        'v2_evaluations': v2_data[0],
                        'v2_avg_success': v2_data[1],
                        'v2_models_evaluated': v2_data[2]
                    }
            except sqlite3.OperationalError:
                v2_metrics = {'v2_evaluations': 0}
            
            # Pattern insights
            try:
                cursor = conn.execute("""
                    SELECT COUNT(*) as patterns_discovered
                    FROM v2_discovered_patterns
                    WHERE discovered_at >= ?
                """, [cutoff_date])
                
                pattern_count = cursor.fetchone()[0] if cursor.fetchone() else 0
            except sqlite3.OperationalError:
                pattern_count = 0
            
            # Recent recommendations
            try:
                cursor = conn.execute("""
                    SELECT COUNT(*) as active_recommendations
                    FROM v2_optimization_insights
                    WHERE created_at >= ? AND applied = FALSE
                """, [cutoff_date])
                
                rec_count = cursor.fetchone()[0] if cursor.fetchone() else 0
            except sqlite3.OperationalError:
                rec_count = 0
        
        # Build comprehensive overview
        overview = {
            'timestamp': datetime.now().isoformat(),
            'time_range': time_range.value,
            'system_status': 'operational',
            
            # Core metrics
            'core_metrics': {
                'total_tasks': core_metrics[0] if core_metrics else 0,
                'avg_success_rate': round((core_metrics[1] or 0) * 100, 1),
                'total_cost_usd': round(core_metrics[2] or 0, 4),
                'avg_latency_ms': round(core_metrics[3] or 0, 0),
                'models_used': core_metrics[4] if core_metrics else 0,
                'last_activity': core_metrics[5] if core_metrics else None
            },
            
            # V2.0 enhanced metrics
            'v2_intelligence': v2_metrics,
            
            # Pattern and insights
            'intelligence_insights': {
                'patterns_discovered': pattern_count,
                'active_recommendations': rec_count,
                'intelligence_status': 'active' if pattern_count > 0 or rec_count > 0 else 'learning'
            },
            
            # Performance trends (simplified)
            'trends': self._calculate_basic_trends(cutoff_date),
            
            # Health indicators
            'health_indicators': {
                'database_health': 'healthy',
                'v2_components': 'operational',
                'pattern_mining': 'active' if pattern_count > 0 else 'standby',
                'ml_pipeline': 'ready'
            }
        }
        
        return overview
    
    def get_metric_data(self, metric_type: MetricType, time_range: TimeRange) -> Dict[str, Any]:
        """Get specific metric data with detailed breakdown"""
        
        days_back = self._get_days_from_range(time_range)
        cutoff_date = (datetime.now() - timedelta(days=days_back)).isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            if metric_type == MetricType.SUCCESS_RATE:
                cursor = conn.execute("""
                    SELECT 
                        DATE(timestamp) as date,
                        AVG(success_score) as avg_success,
                        COUNT(*) as task_count
                    FROM simple_tracker
                    WHERE timestamp >= ?
                    GROUP BY DATE(timestamp)
                    ORDER BY date DESC
                """, [cutoff_date])
                
                data_points = [
                    {
                        'date': row[0],
                        'success_rate': round((row[1] or 0) * 100, 1),
                        'task_count': row[2]
                    }
                    for row in cursor.fetchall()
                ]
                
                return {
                    'metric_type': metric_type.value,
                    'time_range': time_range.value,
                    'data_points': data_points,
                    'summary': {
                        'avg_success_rate': round(sum(d['success_rate'] for d in data_points) / len(data_points), 1) if data_points else 0,
                        'total_tasks': sum(d['task_count'] for d in data_points),
                        'data_points_count': len(data_points)
                    }
                }
            
            elif metric_type == MetricType.COST_ANALYSIS:
                cursor = conn.execute("""
                    SELECT 
                        model_used,
                        SUM(cost_usd) as total_cost,
                        AVG(cost_usd) as avg_cost,
                        COUNT(*) as usage_count
                    FROM simple_tracker
                    WHERE timestamp >= ? AND cost_usd IS NOT NULL
                    GROUP BY model_used
                    ORDER BY total_cost DESC
                """, [cutoff_date])
                
                cost_breakdown = [
                    {
                        'model': row[0],
                        'total_cost': round(row[1], 4),
                        'avg_cost': round(row[2], 4),
                        'usage_count': row[3],
                        'cost_per_success': round(row[1] / max(row[3], 1), 4)
                    }
                    for row in cursor.fetchall()
                ]
                
                total_cost = sum(item['total_cost'] for item in cost_breakdown)
                
                return {
                    'metric_type': metric_type.value,
                    'time_range': time_range.value,
                    'cost_breakdown': cost_breakdown,
                    'summary': {
                        'total_cost': round(total_cost, 4),
                        'models_analyzed': len(cost_breakdown),
                        'most_expensive': cost_breakdown[0]['model'] if cost_breakdown else None,
                        'optimization_potential': round(total_cost * 0.25, 4)  # Estimated 25% savings potential
                    }
                }
            
            elif metric_type == MetricType.MODEL_PERFORMANCE:
                cursor = conn.execute("""
                    SELECT 
                        model_used,
                        task_type,
                        AVG(success_score) as avg_success,
                        AVG(cost_usd) as avg_cost,
                        AVG(latency_ms) as avg_latency,
                        COUNT(*) as usage_count
                    FROM simple_tracker
                    WHERE timestamp >= ?
                    GROUP BY model_used, task_type
                    HAVING COUNT(*) >= 3
                    ORDER BY AVG(success_score) DESC
                """, [cutoff_date])
                
                performance_matrix = [
                    {
                        'model': row[0],
                        'task_type': row[1],
                        'avg_success': round((row[2] or 0) * 100, 1),
                        'avg_cost': round(row[3] or 0, 4),
                        'avg_latency': round(row[4] or 0, 0),
                        'usage_count': row[5],
                        'efficiency_score': round(((row[2] or 0) / max(row[3] or 0.001, 0.001)), 1)
                    }
                    for row in cursor.fetchall()
                ]
                
                return {
                    'metric_type': metric_type.value,
                    'time_range': time_range.value,
                    'performance_matrix': performance_matrix,
                    'summary': {
                        'total_combinations': len(performance_matrix),
                        'best_performer': performance_matrix[0] if performance_matrix else None,
                        'models_evaluated': len(set(item['model'] for item in performance_matrix))
                    }
                }
        
        # Default fallback
        return {
            'metric_type': metric_type.value,
            'time_range': time_range.value,
            'data': 'not_available',
            'message': f'Metric {metric_type.value} not implemented yet'
        }
    
    def get_real_time_summary(self) -> Dict[str, Any]:
        """Get real-time system summary for live monitoring"""
        with sqlite3.connect(self.db_path) as conn:
            # Last 1 hour statistics
            one_hour_ago = (datetime.now() - timedelta(hours=1)).isoformat()
            
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as recent_tasks,
                    AVG(success_score) as recent_success,
                    SUM(cost_usd) as recent_cost,
                    MAX(timestamp) as last_task
                FROM simple_tracker
                WHERE timestamp >= ?
            """, [one_hour_ago])
            
            recent_stats = cursor.fetchone()
            
            # System health indicators
            cursor = conn.execute("""
                SELECT COUNT(*) as total_tasks FROM simple_tracker
            """)
            total_tasks = cursor.fetchone()[0]
            
            # V2.0 components status
            v2_status = {}
            try:
                cursor = conn.execute("SELECT COUNT(*) FROM v2_success_evaluations")
                v2_status['evaluations_count'] = cursor.fetchone()[0]
                
                cursor = conn.execute("SELECT COUNT(*) FROM v2_discovered_patterns")
                v2_status['patterns_count'] = cursor.fetchone()[0]
                
                cursor = conn.execute("SELECT COUNT(*) FROM v2_optimization_insights WHERE applied = FALSE")
                v2_status['active_insights'] = cursor.fetchone()[0]
                
            except sqlite3.OperationalError:
                v2_status = {'status': 'v2_tables_not_found'}
        
        return {
            'timestamp': datetime.now().isoformat(),
            'system_status': 'operational',
            
            # Recent activity (last hour)
            'recent_activity': {
                'tasks_executed': recent_stats[0] if recent_stats else 0,
                'avg_success_rate': round((recent_stats[1] or 0) * 100, 1),
                'cost_spent': round(recent_stats[2] or 0, 4),
                'last_task_time': recent_stats[3] if recent_stats else None,
                'activity_level': 'high' if (recent_stats[0] or 0) > 10 else 'low'
            },
            
            # Overall statistics
            'overall_stats': {
                'total_tasks_processed': total_tasks,
                'system_uptime': 'operational',
                'database_health': 'healthy'
            },
            
            # V2.0 intelligence status
            'v2_intelligence': v2_status,
            
            # Performance indicators
            'performance_indicators': {
                'response_time': 'optimal',
                'throughput': 'normal',
                'error_rate': 'low',
                'resource_usage': 'moderate'
            }
        }
    
    def get_cost_optimization_data(self) -> Dict[str, Any]:
        """Get comprehensive cost optimization analytics"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT 
                    model_used,
                    SUM(cost_usd) as total_spent,
                    AVG(cost_usd) as avg_cost,
                    AVG(success_score) as avg_success,
                    COUNT(*) as usage_count
                FROM simple_tracker
                WHERE cost_usd IS NOT NULL
                AND timestamp >= ?
                GROUP BY model_used
                ORDER BY total_spent DESC
            """, [(datetime.now() - timedelta(days=30)).isoformat()])
            
            model_costs = cursor.fetchall()
            
            cost_data = []
            total_spent = 0
            
            for row in model_costs:
                model, total_cost, avg_cost, avg_success, usage = row
                cost_efficiency = (avg_success or 0) / max(avg_cost, 0.001)
                
                total_spent += total_cost
                cost_data.append({
                    'model': model,
                    'total_cost': round(total_cost, 4),
                    'avg_cost': round(avg_cost, 4),
                    'avg_success': round((avg_success or 0) * 100, 1),
                    'usage_count': usage,
                    'cost_efficiency': round(cost_efficiency, 1),
                    'spending_share': 0  # Will calculate below
                })
            
            # Calculate spending share
            for item in cost_data:
                item['spending_share'] = round((item['total_cost'] / max(total_spent, 0.001)) * 100, 1)
            
            # Optimization opportunities
            high_cost_models = [item for item in cost_data if item['avg_cost'] > 0.015]
            optimization_potential = sum(item['total_cost'] for item in high_cost_models) * 0.3
            
            return {
                'timestamp': datetime.now().isoformat(),
                'cost_analysis': cost_data,
                'summary': {
                    'total_spent_30d': round(total_spent, 4),
                    'models_analyzed': len(cost_data),
                    'most_expensive_model': cost_data[0]['model'] if cost_data else None,
                    'optimization_potential': round(optimization_potential, 4),
                    'estimated_monthly_savings': round(optimization_potential, 4)
                },
                'optimization_recommendations': [
                    {
                        'type': 'cost_reduction',
                        'description': 'Use local models for routine tasks',
                        'estimated_savings': round(optimization_potential * 0.6, 4)
                    },
                    {
                        'type': 'efficiency_improvement', 
                        'description': 'Batch similar requests to reduce overhead',
                        'estimated_savings': round(optimization_potential * 0.4, 4)
                    }
                ]
            }
    
    def get_pattern_insights_data(self) -> Dict[str, Any]:
        """Get pattern mining insights for dashboard"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT 
                        pattern_type, name, confidence, sample_size,
                        impact_metrics, recommendations, discovered_at
                    FROM v2_discovered_patterns
                    ORDER BY confidence DESC, discovered_at DESC
                    LIMIT 20
                """)
                
                patterns = []
                for row in cursor.fetchall():
                    pattern_type, name, confidence, sample_size, impact_metrics, recommendations, discovered_at = row
                    
                    try:
                        impact_data = json.loads(impact_metrics)
                        rec_data = json.loads(recommendations)
                    except:
                        impact_data = {}
                        rec_data = []
                    
                    patterns.append({
                        'type': pattern_type,
                        'name': name,
                        'confidence': round(confidence, 3),
                        'sample_size': sample_size,
                        'impact_metrics': impact_data,
                        'recommendations': rec_data,
                        'discovered_at': discovered_at
                    })
                
                return {
                    'timestamp': datetime.now().isoformat(),
                    'patterns_discovered': patterns,
                    'summary': {
                        'total_patterns': len(patterns),
                        'high_confidence_patterns': len([p for p in patterns if p['confidence'] > 0.8]),
                        'recent_discoveries': len([p for p in patterns 
                                                if datetime.fromisoformat(p['discovered_at']) > 
                                                datetime.now() - timedelta(days=7)])
                    }
                }
        
        except sqlite3.OperationalError:
            return {
                'timestamp': datetime.now().isoformat(),
                'patterns_discovered': [],
                'summary': {'total_patterns': 0, 'status': 'v2_tables_not_ready'}
            }
    
    def get_model_performance_data(self) -> Dict[str, Any]:
        """Get detailed model performance analytics"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT 
                    model_used,
                    task_type,
                    COUNT(*) as executions,
                    AVG(success_score) as avg_success,
                    AVG(cost_usd) as avg_cost,
                    AVG(latency_ms) as avg_latency,
                    MIN(success_score) as min_success,
                    MAX(success_score) as max_success
                FROM simple_tracker
                WHERE timestamp >= ?
                AND success_score IS NOT NULL
                GROUP BY model_used, task_type
                HAVING COUNT(*) >= 3
                ORDER BY AVG(success_score) DESC
            """, [(datetime.now() - timedelta(days=30)).isoformat()])
            
            performance_data = []
            for row in cursor.fetchall():
                model, task_type, executions, avg_success, avg_cost, avg_latency, min_success, max_success = row
                
                # Calculate performance score
                success_score = (avg_success or 0) * 0.5
                cost_score = min(1.0, 0.02 / max(avg_cost or 0.001, 0.001)) * 0.3
                latency_score = min(1.0, 2000 / max(avg_latency or 1, 1)) * 0.2
                overall_score = success_score + cost_score + latency_score
                
                performance_data.append({
                    'model': model,
                    'task_type': task_type,
                    'executions': executions,
                    'avg_success': round((avg_success or 0) * 100, 1),
                    'avg_cost': round(avg_cost or 0, 4),
                    'avg_latency': round(avg_latency or 0, 0),
                    'success_range': {
                        'min': round((min_success or 0) * 100, 1),
                        'max': round((max_success or 0) * 100, 1)
                    },
                    'overall_score': round(overall_score, 3),
                    'recommendation': 'excellent' if overall_score > 0.8 else 'good' if overall_score > 0.6 else 'needs_improvement'
                })
            
            return {
                'timestamp': datetime.now().isoformat(),
                'model_performance': performance_data,
                'summary': {
                    'combinations_analyzed': len(performance_data),
                    'best_performer': performance_data[0] if performance_data else None,
                    'models_count': len(set(item['model'] for item in performance_data)),
                    'task_types_count': len(set(item['task_type'] for item in performance_data))
                }
            }
    
    def _calculate_basic_trends(self, cutoff_date: str) -> Dict[str, Any]:
        """Calculate basic trends for overview"""
        with sqlite3.connect(self.db_path) as conn:
            # Task volume trend
            cursor = conn.execute("""
                SELECT 
                    DATE(timestamp) as date,
                    COUNT(*) as task_count,
                    AVG(success_score) as avg_success
                FROM simple_tracker
                WHERE timestamp >= ?
                GROUP BY DATE(timestamp)
                ORDER BY date ASC
            """, [cutoff_date])
            
            daily_data = cursor.fetchall()
            
            if len(daily_data) < 2:
                return {'status': 'insufficient_data'}
            
            # Simple trend calculation
            recent_tasks = daily_data[-1][1] if daily_data else 0
            previous_tasks = daily_data[-2][1] if len(daily_data) > 1 else recent_tasks
            
            task_trend = "up" if recent_tasks > previous_tasks else "down" if recent_tasks < previous_tasks else "stable"
            
            # Success rate trend
            recent_success = daily_data[-1][2] or 0
            previous_success = daily_data[-2][2] if len(daily_data) > 1 else recent_success
            
            success_trend = "up" if recent_success > previous_success else "down" if recent_success < previous_success else "stable"
            
            return {
                'task_volume': {
                    'trend': task_trend,
                    'recent_count': recent_tasks,
                    'change': recent_tasks - previous_tasks
                },
                'success_rate': {
                    'trend': success_trend,
                    'recent_rate': round(recent_success * 100, 1),
                    'change_points': round((recent_success - previous_success) * 100, 1)
                }
            }
    
    def _get_days_from_range(self, time_range: TimeRange) -> int:
        """Convert TimeRange enum to days"""
        range_map = {
            TimeRange.HOUR: 0.04,  # ~1 hour
            TimeRange.DAY: 1,
            TimeRange.WEEK: 7,
            TimeRange.MONTH: 30,
            TimeRange.QUARTER: 90
        }
        return range_map.get(time_range, 1)
    
    async def _refresh_analytics_cache(self):
        """Background task to refresh analytics cache"""
        logger.info("ðŸ”„ Refreshing analytics cache...")
        
        # In production, this would refresh cached data
        # For now, just log the refresh
        await asyncio.sleep(2)  # Simulate processing time
        
        logger.info("âœ… Analytics cache refreshed")
    
    async def broadcast_to_websockets(self, data: Dict[str, Any]):
        """Broadcast data to all connected WebSocket clients"""
        if not self.active_connections:
            return
        
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_json(data)
            except Exception as e:
                logger.warning(f"WebSocket send failed: {e}")
                disconnected.append(connection)
        
        # Clean up disconnected clients
        for conn in disconnected:
            self.active_connections.remove(conn)

# CLI Integration Functions
def start_analytics_api(host: str = "0.0.0.0", port: int = 8003) -> None:
    """CLI function to start analytics API server"""
    if not FASTAPI_AVAILABLE:
        logger.error("âŒ FastAPI not available - cannot start server")
        return
    
    dashboard_api = AnalyticsDashboardAPI()
    
    if dashboard_api.app:
        import uvicorn
        logger.info(f"ðŸš€ Starting Analytics API on {host}:{port}")
        uvicorn.run(dashboard_api.app, host=host, port=port)

def get_dashboard_data(time_range: str = "24h") -> Dict[str, Any]:
    """CLI function to get dashboard data"""
    api = AnalyticsDashboardAPI()
    return api.get_analytics_overview(TimeRange(time_range))

def get_analytics_health_check() -> Dict[str, Any]:
    """CLI function to check analytics system health"""
    api = AnalyticsDashboardAPI()
    
    health_data = {
        'timestamp': datetime.now().isoformat(),
        'api_status': 'available' if FASTAPI_AVAILABLE else 'unavailable',
        'database_accessible': True,
        'v2_components': 'checking...'
    }
    
    # Test database access
    try:
        with sqlite3.connect(api.db_path) as conn:
            cursor = conn.execute("SELECT COUNT(*) FROM simple_tracker")
            task_count = cursor.fetchone()[0]
            health_data['database_accessible'] = True
            health_data['total_tasks'] = task_count
    except Exception as e:
        health_data['database_accessible'] = False
        health_data['database_error'] = str(e)
    
    # Test V2.0 components
    try:
        with sqlite3.connect(api.db_path) as conn:
            v2_tables = ['v2_success_evaluations', 'v2_discovered_patterns', 'v2_optimization_insights']
            v2_status = {}
            
            for table in v2_tables:
                cursor = conn.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                v2_status[table] = count
            
            health_data['v2_components'] = v2_status
    except sqlite3.OperationalError:
        health_data['v2_components'] = 'not_ready'
    
    return health_data

def generate_analytics_report(format: str = "json") -> Dict[str, Any]:
    """CLI function to generate comprehensive analytics report"""
    api = AnalyticsDashboardAPI()
    
    report = {
        'report_generated': datetime.now().isoformat(),
        'overview': api.get_analytics_overview(TimeRange.WEEK),
        'cost_analysis': api.get_cost_optimization_data(),
        'pattern_insights': api.get_pattern_insights_data(),
        'model_performance': api.get_model_performance_data(),
        'real_time_summary': api.get_real_time_summary()
    }
    
    if format == "summary":
        return {
            'timestamp': report['report_generated'],
            'total_tasks': report['overview']['core_metrics']['total_tasks'],
            'avg_success_rate': report['overview']['core_metrics']['avg_success_rate'],
            'total_cost': report['overview']['core_metrics']['total_cost_usd'],
            'patterns_discovered': report['pattern_insights']['summary']['total_patterns'],
            'cost_optimization_potential': report['cost_analysis']['summary']['optimization_potential'],
            'system_health': 'operational'
        }
    
    return report

if __name__ == "__main__":
    # Test Enhanced Analytics Dashboard Backend
    print("ðŸ“Š Testing Enhanced Analytics Dashboard Backend...")
    
    # Health check
    health = get_analytics_health_check()
    print(f"âœ… Health Check: API {'available' if health['api_status'] == 'available' else 'unavailable'}")
    
    # Get dashboard data
    dashboard_data = get_dashboard_data("24h")
    print(f"ðŸ“ˆ Dashboard Data: {dashboard_data['core_metrics']['total_tasks']} tasks analyzed")
    
    # Generate report
    report = generate_analytics_report("summary")
    print(f"ðŸ“‹ Analytics Report: {report['avg_success_rate']}% success rate")
    
    print("\nðŸŽ‰ Enhanced Analytics Dashboard Backend - OPERATIONAL!")