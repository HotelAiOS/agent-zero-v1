#!/usr/bin/env python3
"""
Agent Zero V1 - Complete Integration Test Suite
V2.0 Intelligence Layer - Week 44 Implementation

ğŸ¯ Week 44 Critical Task: Complete Integration Test Suite
Zadanie: Comprehensive testing framework for V2.0 components
Rezultat: Automated validation of entire V2.0 Intelligence Layer
Impact: Ensures production-ready deployment with full test coverage

Author: Developer A (Backend Architect)
Date: 10 paÅºdziernika 2025
Linear Issue: A0-44 (Week 44 Implementation)
"""

import unittest
import sqlite3
import json
import os
import tempfile
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import logging
import asyncio

# Configure logging for tests
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestV2ComponentAvailability(unittest.TestCase):
    """Test availability and basic functionality of V2.0 components"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
        self.test_db_path = self.test_db.name
        self.test_db.close()
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_db_path):
            os.unlink(self.test_db_path)
    
    def test_experience_manager_import(self):
        """Test Experience Manager component import and basic functionality"""
        try:
            from shared.experience_manager import ExperienceManager, TaskOutcome
            
            # Initialize with test database
            exp_manager = ExperienceManager(self.test_db_path)
            
            # Test basic functionality
            experience = exp_manager.record_experience(
                task_id="test_001",
                task_type="test",
                context={"test": True},
                outcome=TaskOutcome.SUCCESS,
                success_score=0.9,
                cost_usd=0.01,
                latency_ms=1000,
                model_used="test_model"
            )
            
            self.assertIsNotNone(experience)
            self.assertEqual(experience.task_id, "test_001")
            logger.info("âœ… Experience Manager test passed")
            
        except ImportError as e:
            self.skipTest(f"Experience Manager not available: {e}")
    
    def test_knowledge_graph_import(self):
        """Test Knowledge Graph component import and basic functionality"""
        try:
            from shared.knowledge.neo4j_knowledge_graph import KnowledgeGraphManager
            
            # Test initialization (will use mock if Neo4j not available)
            kg_manager = KnowledgeGraphManager()
            
            # Test basic functionality
            stats = kg_manager.get_graph_statistics()
            self.assertIsInstance(stats, dict)
            
            logger.info("âœ… Knowledge Graph Manager test passed")
            
        except ImportError as e:
            self.skipTest(f"Knowledge Graph Manager not available: {e}")
    
    def test_pattern_mining_import(self):
        """Test Pattern Mining Engine component import and basic functionality"""
        try:
            from shared.learning.pattern_mining_engine import PatternMiningEngine
            
            # Initialize with test database
            engine = PatternMiningEngine(self.test_db_path)
            
            # Test basic functionality
            patterns = engine.get_stored_patterns()
            self.assertIsInstance(patterns, list)
            
            logger.info("âœ… Pattern Mining Engine test passed")
            
        except ImportError as e:
            self.skipTest(f"Pattern Mining Engine not available: {e}")
    
    def test_ml_pipeline_import(self):
        """Test ML Training Pipeline component import and basic functionality"""
        try:
            from shared.learning.ml_training_pipeline import MLModelTrainingPipeline
            
            # Initialize with test database
            pipeline = MLModelTrainingPipeline(self.test_db_path)
            
            # Test basic functionality
            training_data = pipeline.prepare_training_data(days_back=7)
            self.assertIsInstance(training_data, dict)
            
            logger.info("âœ… ML Training Pipeline test passed")
            
        except ImportError as e:
            self.skipTest(f"ML Training Pipeline not available: {e}")
    
    def test_analytics_dashboard_import(self):
        """Test Analytics Dashboard API component import and basic functionality"""
        try:
            from api.analytics_dashboard_api import AnalyticsDashboardAPI
            
            # Initialize with test database
            api = AnalyticsDashboardAPI(self.test_db_path)
            
            # Test basic functionality
            overview = api.get_analytics_overview()
            self.assertIsInstance(overview, dict)
            self.assertIn('timestamp', overview)
            
            logger.info("âœ… Analytics Dashboard API test passed")
            
        except ImportError as e:
            self.skipTest(f"Analytics Dashboard API not available: {e}")
    
    def test_enhanced_tracker_import(self):
        """Test Enhanced SimpleTracker component import and basic functionality"""
        try:
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker, TrackingLevel
            
            # Initialize with test database
            tracker = EnhancedSimpleTracker(self.test_db_path)
            
            # Test basic functionality
            task_id = "test_tracker_001"
            result = tracker.track_event(
                task_id=task_id,
                task_type="test",
                model_used="test_model",
                success_score=0.8,
                tracking_level=TrackingLevel.FULL
            )
            
            self.assertEqual(result, task_id)
            logger.info("âœ… Enhanced SimpleTracker test passed")
            
        except ImportError as e:
            self.skipTest(f"Enhanced SimpleTracker not available: {e}")

class TestV2DatabaseSchema(unittest.TestCase):
    """Test V2.0 database schema and data integrity"""
    
    def setUp(self):
        """Set up test database"""
        self.test_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
        self.test_db_path = self.test_db.name
        self.test_db.close()
    
    def tearDown(self):
        """Clean up test database"""
        if os.path.exists(self.test_db_path):
            os.unlink(self.test_db_path)
    
    def test_v2_tables_creation(self):
        """Test creation of V2.0 database tables"""
        # Initialize Enhanced SimpleTracker to create tables
        try:
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker
            tracker = EnhancedSimpleTracker(self.test_db_path)
            
            # Check that V2.0 tables exist
            with sqlite3.connect(self.test_db_path) as conn:
                cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                v2_tables = [
                    'v2_enhanced_tracker',
                    'v2_success_evaluations', 
                    'v2_model_decisions',
                    'v2_system_alerts'
                ]
                
                for table in v2_tables:
                    self.assertIn(table, tables, f"V2.0 table {table} not created")
                
                logger.info(f"âœ… V2.0 database schema test passed: {len(v2_tables)} tables verified")
        
        except ImportError:
            self.skipTest("Enhanced SimpleTracker not available")
    
    def test_experience_manager_tables(self):
        """Test Experience Manager database tables"""
        try:
            from shared.experience_manager import ExperienceManager
            exp_manager = ExperienceManager(self.test_db_path)
            
            with sqlite3.connect(self.test_db_path) as conn:
                cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                exp_tables = ['v2_experiences', 'v2_patterns', 'v2_recommendations']
                
                for table in exp_tables:
                    self.assertIn(table, tables, f"Experience Manager table {table} not created")
                
                logger.info("âœ… Experience Manager database schema test passed")
        
        except ImportError:
            self.skipTest("Experience Manager not available")
    
    def test_pattern_mining_tables(self):
        """Test Pattern Mining Engine database tables"""
        try:
            from shared.learning.pattern_mining_engine import PatternMiningEngine
            engine = PatternMiningEngine(self.test_db_path)
            
            with sqlite3.connect(self.test_db_path) as conn:
                cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                pattern_tables = ['v2_discovered_patterns', 'v2_optimization_insights']
                
                for table in pattern_tables:
                    self.assertIn(table, tables, f"Pattern Mining table {table} not created")
                
                logger.info("âœ… Pattern Mining database schema test passed")
        
        except ImportError:
            self.skipTest("Pattern Mining Engine not available")
    
    def test_ml_pipeline_tables(self):
        """Test ML Training Pipeline database tables"""
        try:
            from shared.learning.ml_training_pipeline import MLModelTrainingPipeline
            pipeline = MLModelTrainingPipeline(self.test_db_path)
            
            with sqlite3.connect(self.test_db_path) as conn:
                cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                ml_tables = ['v2_training_jobs', 'v2_ml_predictions']
                
                for table in ml_tables:
                    self.assertIn(table, tables, f"ML Pipeline table {table} not created")
                
                logger.info("âœ… ML Pipeline database schema test passed")
        
        except ImportError:
            self.skipTest("ML Training Pipeline not available")

class TestV2DataFlow(unittest.TestCase):
    """Test data flow between V2.0 components"""
    
    def setUp(self):
        """Set up test environment with sample data"""
        self.test_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
        self.test_db_path = self.test_db.name
        self.test_db.close()
        
        # Create sample data
        self._create_sample_data()
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_db_path):
            os.unlink(self.test_db_path)
    
    def _create_sample_data(self):
        """Create sample data for testing"""
        try:
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker, TrackingLevel
            
            tracker = EnhancedSimpleTracker(self.test_db_path)
            
            # Create sample tracking data
            sample_tasks = [
                {
                    'task_id': 'test_flow_001',
                    'task_type': 'code_generation',
                    'model_used': 'llama3.2-3b',
                    'success_score': 0.92,
                    'cost_usd': 0.015,
                    'latency_ms': 1200
                },
                {
                    'task_id': 'test_flow_002', 
                    'task_type': 'text_analysis',
                    'model_used': 'qwen2.5-3b',
                    'success_score': 0.85,
                    'cost_usd': 0.012,
                    'latency_ms': 800
                },
                {
                    'task_id': 'test_flow_003',
                    'task_type': 'code_generation',
                    'model_used': 'llama3.2-3b',
                    'success_score': 0.88,
                    'cost_usd': 0.018,
                    'latency_ms': 1500
                }
            ]
            
            for task in sample_tasks:
                tracker.track_event(
                    tracking_level=TrackingLevel.FULL,
                    **task
                )
            
            logger.info(f"âœ… Sample data created: {len(sample_tasks)} tasks")
        
        except ImportError:
            logger.warning("Could not create sample data - Enhanced SimpleTracker not available")
    
    def test_experience_recording_flow(self):
        """Test flow from tracking to experience recording"""
        try:
            from shared.experience_manager import ExperienceManager
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker
            
            # Verify tracking data exists
            tracker = EnhancedSimpleTracker(self.test_db_path)
            summary = tracker.get_enhanced_summary()
            
            self.assertGreater(summary['v1_metrics']['total_tasks'], 0, "No sample data found")
            
            # Test experience manager integration
            exp_manager = ExperienceManager(self.test_db_path)
            experiences = exp_manager.get_experiences(days_back=1)
            
            # Experience recording should have been triggered by tracking
            logger.info(f"âœ… Experience recording flow test: {len(experiences)} experiences found")
        
        except ImportError:
            self.skipTest("Required components not available")
    
    def test_pattern_discovery_flow(self):
        """Test pattern discovery from tracking data"""
        try:
            from shared.learning.pattern_mining_engine import PatternMiningEngine
            
            engine = PatternMiningEngine(self.test_db_path)
            
            # Run pattern discovery
            model_patterns = engine.mine_model_performance_patterns(days_back=1)
            cost_patterns = engine.mine_cost_efficiency_patterns(days_back=1)
            
            # Should discover patterns from sample data
            total_patterns = len(model_patterns) + len(cost_patterns)
            logger.info(f"âœ… Pattern discovery flow test: {total_patterns} patterns discovered")
            
        except ImportError:
            self.skipTest("Pattern Mining Engine not available")
    
    def test_analytics_data_flow(self):
        """Test analytics dashboard data aggregation"""
        try:
            from api.analytics_dashboard_api import AnalyticsDashboardAPI
            
            api = AnalyticsDashboardAPI(self.test_db_path)
            
            # Test analytics overview
            overview = api.get_analytics_overview()
            self.assertIn('core_metrics', overview)
            self.assertGreater(overview['core_metrics']['total_tasks'], 0)
            
            # Test cost analysis
            cost_data = api.get_cost_optimization_data()
            self.assertIn('cost_analysis', cost_data)
            
            logger.info("âœ… Analytics data flow test passed")
            
        except ImportError:
            self.skipTest("Analytics Dashboard API not available")

class TestV2IntegrationStability(unittest.TestCase):
    """Test stability and error handling of V2.0 integration"""
    
    def setUp(self):
        """Set up test environment"""
        self.test_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
        self.test_db_path = self.test_db.name
        self.test_db.close()
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_db_path):
            os.unlink(self.test_db_path)
    
    def test_v1_backward_compatibility(self):
        """Test that V1.0 functionality remains unchanged"""
        try:
            from shared.utils.enhanced_simple_tracker import track_event
            
            # Test V1.0 style call
            task_id = track_event("test", "V1 compatibility test", priority="high")
            self.assertIsNotNone(task_id)
            
            # Verify data in V1.0 table
            with sqlite3.connect(self.test_db_path) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM simple_tracker")
                count = cursor.fetchone()[0]
                self.assertGreater(count, 0, "V1.0 data not recorded")
            
            logger.info("âœ… V1.0 backward compatibility test passed")
            
        except ImportError:
            self.skipTest("Enhanced SimpleTracker not available")
    
    def test_component_failure_resilience(self):
        """Test system resilience when individual components fail"""
        try:
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker, TrackingLevel
            
            tracker = EnhancedSimpleTracker(self.test_db_path)
            
            # Test tracking with V2.0 level even if some components are missing
            task_id = tracker.track_event(
                task_id="resilience_test_001",
                task_type="test",
                model_used="test_model",
                success_score=0.8,
                tracking_level=TrackingLevel.FULL  # Should not fail even if components missing
            )
            
            self.assertEqual(task_id, "resilience_test_001")
            logger.info("âœ… Component failure resilience test passed")
            
        except ImportError:
            self.skipTest("Enhanced SimpleTracker not available")
    
    def test_database_error_handling(self):
        """Test error handling for database issues"""
        # Test with invalid database path
        invalid_db_path = "/invalid/path/test.db"
        
        try:
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker
            
            # This should handle the error gracefully
            with self.assertLogs(level='ERROR') as log:
                try:
                    tracker = EnhancedSimpleTracker(invalid_db_path)
                except:
                    pass  # Expected to fail
            
            logger.info("âœ… Database error handling test passed")
            
        except ImportError:
            self.skipTest("Enhanced SimpleTracker not available")
    
    def test_concurrent_access(self):
        """Test concurrent access to V2.0 components"""
        try:
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker, TrackingLevel
            import threading
            import time
            
            tracker = EnhancedSimpleTracker(self.test_db_path)
            results = []
            
            def track_concurrent_task(task_num):
                try:
                    task_id = f"concurrent_test_{task_num:03d}"
                    result = tracker.track_event(
                        task_id=task_id,
                        task_type="concurrent_test",
                        model_used="test_model",
                        success_score=0.7 + (task_num % 3) * 0.1,
                        tracking_level=TrackingLevel.ENHANCED
                    )
                    results.append(result)
                except Exception as e:
                    results.append(f"ERROR: {e}")
            
            # Create multiple threads
            threads = []
            for i in range(10):
                thread = threading.Thread(target=track_concurrent_task, args=(i,))
                threads.append(thread)
                thread.start()
            
            # Wait for all threads to complete
            for thread in threads:
                thread.join()
            
            # Check results
            successful_results = [r for r in results if not str(r).startswith("ERROR")]
            self.assertEqual(len(successful_results), 10, "Some concurrent operations failed")
            
            logger.info(f"âœ… Concurrent access test passed: {len(successful_results)}/10 operations successful")
            
        except ImportError:
            self.skipTest("Enhanced SimpleTracker not available")

class TestV2PerformanceMetrics(unittest.TestCase):
    """Test performance characteristics of V2.0 components"""
    
    def setUp(self):
        """Set up performance test environment"""
        self.test_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
        self.test_db_path = self.test_db.name
        self.test_db.close()
    
    def tearDown(self):
        """Clean up test environment"""
        if os.path.exists(self.test_db_path):
            os.unlink(self.test_db_path)
    
    def test_tracking_performance(self):
        """Test performance of enhanced tracking operations"""
        try:
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker, TrackingLevel
            
            tracker = EnhancedSimpleTracker(self.test_db_path)
            
            # Performance test - track 100 events
            start_time = time.time()
            
            for i in range(100):
                tracker.track_event(
                    task_id=f"perf_test_{i:03d}",
                    task_type="performance_test",
                    model_used="test_model",
                    success_score=0.8,
                    tracking_level=TrackingLevel.FULL
                )
            
            end_time = time.time()
            total_time = end_time - start_time
            ops_per_second = 100 / total_time
            
            # Performance expectation: should handle at least 10 ops/second
            self.assertGreater(ops_per_second, 10, f"Performance too slow: {ops_per_second:.2f} ops/sec")
            
            logger.info(f"âœ… Tracking performance test passed: {ops_per_second:.2f} ops/sec")
            
        except ImportError:
            self.skipTest("Enhanced SimpleTracker not available")
    
    def test_analytics_query_performance(self):
        """Test performance of analytics queries"""
        try:
            from api.analytics_dashboard_api import AnalyticsDashboardAPI
            from shared.utils.enhanced_simple_tracker import EnhancedSimpleTracker, TrackingLevel
            
            # Create test data
            tracker = EnhancedSimpleTracker(self.test_db_path)
            for i in range(50):
                tracker.track_event(
                    task_id=f"analytics_perf_{i:03d}",
                    task_type="analytics_test",
                    model_used=f"model_{i % 3}",
                    success_score=0.7 + (i % 4) * 0.1,
                    cost_usd=0.01 + (i % 5) * 0.005,
                    latency_ms=1000 + (i % 10) * 100,
                    tracking_level=TrackingLevel.FULL
                )
            
            # Test analytics performance
            api = AnalyticsDashboardAPI(self.test_db_path)
            
            start_time = time.time()
            overview = api.get_analytics_overview()
            cost_data = api.get_cost_optimization_data()
            model_performance = api.get_model_performance_data()
            end_time = time.time()
            
            query_time = end_time - start_time
            
            # Performance expectation: analytics queries should complete within 2 seconds
            self.assertLess(query_time, 2.0, f"Analytics queries too slow: {query_time:.2f}s")
            
            logger.info(f"âœ… Analytics query performance test passed: {query_time:.3f}s")
            
        except ImportError:
            self.skipTest("Analytics Dashboard API not available")

class V2IntegrationTestSuite:
    """Complete V2.0 Integration Test Suite Runner"""
    
    def __init__(self):
        self.test_results = {}
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.skipped_tests = 0
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Run complete V2.0 integration test suite"""
        
        logger.info("ğŸ§ª Starting Agent Zero V2.0 Complete Integration Test Suite")
        
        test_classes = [
            TestV2ComponentAvailability,
            TestV2DatabaseSchema,
            TestV2DataFlow,
            TestV2IntegrationStability,
            TestV2PerformanceMetrics
        ]
        
        overall_results = {
            'test_suite_started': datetime.now().isoformat(),
            'test_classes': len(test_classes),
            'test_results': {},
            'summary': {}
        }
        
        for test_class in test_classes:
            class_name = test_class.__name__
            logger.info(f"ğŸ” Running {class_name}...")
            
            # Run test class
            suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
            runner = unittest.TextTestRunner(verbosity=0, stream=open(os.devnull, 'w'))
            result = runner.run(suite)
            
            # Collect results
            class_results = {
                'tests_run': result.testsRun,
                'failures': len(result.failures),
                'errors': len(result.errors),
                'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0,
                'success_rate': ((result.testsRun - len(result.failures) - len(result.errors)) / max(result.testsRun, 1)) * 100
            }
            
            overall_results['test_results'][class_name] = class_results
            
            # Update totals
            self.total_tests += result.testsRun
            self.passed_tests += result.testsRun - len(result.failures) - len(result.errors)
            self.failed_tests += len(result.failures) + len(result.errors)
            self.skipped_tests += len(result.skipped) if hasattr(result, 'skipped') else 0
            
            logger.info(f"  âœ… {class_name}: {class_results['success_rate']:.1f}% success rate")
        
        # Calculate overall summary
        overall_success_rate = (self.passed_tests / max(self.total_tests, 1)) * 100
        
        overall_results['summary'] = {
            'total_tests': self.total_tests,
            'passed_tests': self.passed_tests,
            'failed_tests': self.failed_tests,
            'skipped_tests': self.skipped_tests,
            'overall_success_rate': round(overall_success_rate, 1),
            'test_status': 'PASSED' if overall_success_rate >= 80 else 'FAILED',
            'completed_at': datetime.now().isoformat()
        }
        
        return overall_results
    
    def generate_test_report(self, results: Dict[str, Any]) -> str:
        """Generate comprehensive test report"""
        
        report = f"""
# Agent Zero V2.0 Integration Test Report

**Test Suite Executed:** {results['test_suite_started']}
**Test Classes:** {results['test_classes']}
**Overall Status:** {results['summary']['test_status']}

## Summary
- **Total Tests:** {results['summary']['total_tests']}
- **Passed:** {results['summary']['passed_tests']}
- **Failed:** {results['summary']['failed_tests']}
- **Skipped:** {results['summary']['skipped_tests']}
- **Success Rate:** {results['summary']['overall_success_rate']}%

## Test Class Results

"""
        
        for class_name, class_results in results['test_results'].items():
            status_emoji = "âœ…" if class_results['success_rate'] >= 80 else "âŒ"
            report += f"""
### {status_emoji} {class_name}
- Tests Run: {class_results['tests_run']}
- Success Rate: {class_results['success_rate']:.1f}%
- Failures: {class_results['failures']}
- Errors: {class_results['errors']}
- Skipped: {class_results['skipped']}
"""
        
        report += f"""

## Recommendations

"""
        
        if results['summary']['overall_success_rate'] >= 90:
            report += "ğŸ‰ **Excellent!** V2.0 Integration is production-ready with high test coverage.\n"
        elif results['summary']['overall_success_rate'] >= 80:
            report += "âœ… **Good!** V2.0 Integration is ready for deployment with minor issues to address.\n"
        else:
            report += "âš ï¸ **Attention Required!** Some V2.0 components need fixes before production deployment.\n"
        
        report += f"""
**Test Completed:** {results['summary']['completed_at']}
"""
        
        return report

# CLI Integration Functions
def run_v2_integration_tests() -> Dict[str, Any]:
    """CLI function to run V2.0 integration tests"""
    test_suite = V2IntegrationTestSuite()
    return test_suite.run_all_tests()

def generate_v2_test_report() -> str:
    """CLI function to generate and return test report"""
    test_suite = V2IntegrationTestSuite()
    results = test_suite.run_all_tests()
    return test_suite.generate_test_report(results)

def quick_v2_health_check() -> Dict[str, Any]:
    """CLI function for quick V2.0 health check"""
    health_check = {
        'timestamp': datetime.now().isoformat(),
        'components_checked': 0,
        'components_available': 0,
        'database_status': 'unknown',
        'overall_health': 'unknown'
    }
    
    # Check component imports
    components = [
        ('Experience Manager', 'shared.experience_manager'),
        ('Knowledge Graph', 'shared.knowledge.neo4j_knowledge_graph'),
        ('Pattern Mining', 'shared.learning.pattern_mining_engine'),
        ('ML Pipeline', 'shared.learning.ml_training_pipeline'),
        ('Analytics API', 'api.analytics_dashboard_api'),
        ('Enhanced Tracker', 'shared.utils.enhanced_simple_tracker')
    ]
    
    available_components = []
    
    for comp_name, module_path in components:
        try:
            __import__(module_path)
            available_components.append(comp_name)
        except ImportError:
            pass
    
    health_check['components_checked'] = len(components)
    health_check['components_available'] = len(available_components)
    health_check['available_components'] = available_components
    
    # Database check
    try:
        with sqlite3.connect("agent_zero.db") as conn:
            cursor = conn.execute("SELECT COUNT(*) FROM simple_tracker")
            count = cursor.fetchone()[0]
            health_check['database_status'] = 'healthy'
            health_check['v1_records'] = count
    except Exception as e:
        health_check['database_status'] = f'error: {e}'
    
    # Overall health assessment
    component_ratio = len(available_components) / len(components)
    if component_ratio >= 0.8 and health_check['database_status'] == 'healthy':
        health_check['overall_health'] = 'excellent'
    elif component_ratio >= 0.6:
        health_check['overall_health'] = 'good'
    elif component_ratio >= 0.4:
        health_check['overall_health'] = 'fair'
    else:
        health_check['overall_health'] = 'poor'
    
    return health_check

if __name__ == "__main__":
    # Run complete integration test suite
    print("ğŸ§ª Agent Zero V2.0 Complete Integration Test Suite")
    print("=" * 60)
    
    # Quick health check first
    health = quick_v2_health_check()
    print(f"ğŸ“Š Quick Health Check: {health['overall_health'].upper()}")
    print(f"   Components Available: {health['components_available']}/{health['components_checked']}")
    print(f"   Database Status: {health['database_status']}")
    
    # Run full test suite
    print("\nğŸ” Running Full Integration Tests...")
    test_suite = V2IntegrationTestSuite()
    results = test_suite.run_all_tests()
    
    # Display results
    summary = results['summary']
    print(f"\nğŸ“‹ Test Results Summary:")
    print(f"   Total Tests: {summary['total_tests']}")
    print(f"   Passed: {summary['passed_tests']}")
    print(f"   Failed: {summary['failed_tests']}")  
    print(f"   Success Rate: {summary['overall_success_rate']}%")
    print(f"   Status: {summary['test_status']}")
    
    # Generate report
    report = test_suite.generate_test_report(results)
    print(f"\nğŸ“„ Full test report generated ({len(report)} characters)")
    
    print(f"\nğŸ‰ Agent Zero V2.0 Integration Test Suite - COMPLETE!")