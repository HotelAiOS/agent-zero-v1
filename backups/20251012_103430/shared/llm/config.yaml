# Agent Zero v1 - Multi-LLM Configuration

# Default provider (ollama, openai, anthropic, google)
default_provider: "ollama"

# LLM Base Configuration
llm:
  base_url: "http://localhost:11434"
  
  # Default parameters
  default:
    temperature: 0.7
    num_ctx: 8192
    num_predict: 4096
    max_tokens: 4096
  
  # Agent-specific models (Ollama)
  agents:
    backend:
      model: "deepseek-coder:33b"
      temperature: 0.7
      num_ctx: 8192
    
    frontend:
      model: "deepseek-coder:33b"
      temperature: 0.7
      num_ctx: 8192
    
    architect:
      model: "deepseek-coder:33b"
      temperature: 0.8
      num_ctx: 16384
    
    database:
      model: "deepseek-coder:33b"
      temperature: 0.6
    
    devops:
      model: "deepseek-coder:33b"
      temperature: 0.6
    
    tester:
      model: "deepseek-coder:33b"
      temperature: 0.5
    
    security:
      model: "deepseek-coder:33b"
      temperature: 0.5
    
    performance:
      model: "deepseek-coder:33b"
      temperature: 0.6

# Multi-Provider Configuration
providers:
  # Ollama (local)
  ollama:
    enabled: true
    base_url: "http://localhost:11434"
    models:
      backend: "deepseek-coder:33b"
      frontend: "deepseek-coder:33b"
      architect: "deepseek-coder:33b"
      default: "deepseek-coder:33b"
  
  # OpenAI
  openai:
    enabled: false
    api_key: "${OPENAI_API_KEY}"
    organization: "${OPENAI_ORG_ID}"
    models:
      backend: "gpt-4"
      frontend: "gpt-4"
      architect: "gpt-4o"
      database: "gpt-4"
      default: "gpt-4"
    parameters:
      temperature: 0.7
      max_tokens: 4096
  
  # Anthropic Claude
  anthropic:
    enabled: false
    api_key: "${ANTHROPIC_API_KEY}"
    models:
      backend: "claude-3-5-sonnet-20241022"
      frontend: "claude-3-5-sonnet-20241022"
      architect: "claude-3-5-sonnet-20241022"
      default: "claude-3-5-sonnet-20241022"
    parameters:
      temperature: 0.7
      max_tokens: 4096
  
  # Google Gemini
  google:
    enabled: false
    api_key: "${GOOGLE_API_KEY}"
    models:
      backend: "gemini-1.5-pro"
      frontend: "gemini-1.5-pro"
      architect: "gemini-1.5-pro"
      default: "gemini-1.5-pro"
    parameters:
      temperature: 0.7
      max_tokens: 4096

# Fallback chain (order matters)
fallback_chain:
  - "ollama"
  - "openai"
  - "anthropic"
  - "google"
