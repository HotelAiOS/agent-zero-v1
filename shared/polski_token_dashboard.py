import asyncio
import signal
import sys
import os
from token_streaming_client import token_streaming_ollama
from simple_classifier import classifier
import time

class PolskiTokenDashboard:
    def __init__(self):
        self.running = False
        self.current_task = None
        self.cancel_requested = False
        
    async def uruchom_dashboard(self):
        """Dashboard z polskim interfejsem i live token streaming"""
        
        # WyczyÅ›Ä‡ ekran
        os.system('clear' if os.name == 'posix' else 'cls')
        
        print("ğŸ›ï¸ POLSKI AI DASHBOARD Z LIVE STREAMING")
        print("=" * 70)
        print("âœ¨ OglÄ…daj jak AI myÅ›li sÅ‚owo po sÅ‚owie w czasie rzeczywistym!")
        print("ğŸ§  KaÅ¼dy token pojawia siÄ™ gdy AI go generuje")
        print("ğŸ‡µğŸ‡± Polski interfejs + angielska generacja AI")
        print("=" * 70)
        print()
        print("ğŸ“‹ KOMENDY:")
        print("   â€¢ Wpisz dowolne zadanie aby uruchomiÄ‡ AI")
        print("   â€¢ NaciÅ›nij Ctrl+C aby przerwaÄ‡ generacjÄ™")
        print("   â€¢ Wpisz 'status' aby sprawdziÄ‡ system")
        print("   â€¢ Wpisz 'pomoc' aby pokazaÄ‡ pomoc")
        print("   â€¢ Wpisz 'koniec' aby wyjÅ›Ä‡")
        print("=" * 70)
        
        # ObsÅ‚uga Ctrl+C
        signal.signal(signal.SIGINT, self._obsluz_przerwanie)
        
        while True:
            try:
                print()
                komenda = input("ğŸ›ï¸ OPERATOR > ").strip()
                
                if not komenda:
                    continue
                    
                if komenda.lower() in ['koniec', 'quit', 'wyjÅ›cie', 'exit']:
                    print("\nğŸ‘‹ Zamykam dashboard...")
                    break
                    
                elif komenda.lower() in ['status', 'stan']:
                    self._pokaz_status()
                    
                elif komenda.lower() in ['pomoc', 'help', 'h']:
                    self._pokaz_pomoc()
                    
                else:
                    # Uruchom live streaming AI
                    await self._uruchom_live_streaming(komenda)
                    
            except KeyboardInterrupt:
                print("\nğŸ›‘ Generacja przerwana przez uÅ¼ytkownika")
                self.cancel_requested = True
                continue
                
            except Exception as e:
                print(f"âŒ BÅ‚Ä…d dashboard: {e}")
        
        print("\nğŸšª Dashboard zamkniÄ™ty! Do widzenia!")
    
    async def _uruchom_live_streaming(self, zadanie: str):
        """Uruchom AI z live token streaming po polsku"""
        
        print(f"\nğŸš€ ROZPOCZYNAM LIVE STREAMING AI")
        print("=" * 70)
        print(f"ğŸ“‹ Zadanie: {zadanie}")
        print("=" * 70)
        
        self.current_task = zadanie
        self.cancel_requested = False
        start_time = time.time()
        
        # Krok 1: Analiza zadania
        print("ğŸ¯ AnalizujÄ™ zadanie i wybieram model AI...")
        classification = classifier.classify_with_ai(zadanie)
        selected_model = classification['model']
        
        # Model info po polsku
        model_info = {
            "phi3:mini": "szybki model (2.2GB) - proste zadania", 
            "qwen2.5:14b": "Å›redni model (9GB) - ogÃ³lne zadania",
            "deepseek-coder:33b": "kod specialist (18GB) - programowanie",
            "deepseek-r1:32b": "myÅ›liciel (19GB) - zÅ‚oÅ¼one analizy", 
            "mixtral:8x7b": "kreatywny (26GB) - pisanie i dokumentacja"
        }
        
        print(f"ğŸ¤– Wybrany model: {selected_model}")
        print(f"ğŸ“Š Opis: {model_info.get(selected_model, 'nieznany model')}")
        print(f"ğŸ¯ ZÅ‚oÅ¼onoÅ›Ä‡ zadania: {classification['complexity']}")
        print()
        print("ğŸ”¥ LIVE GENERACJA AI (token po token):")
        print("-" * 70)
        print("ğŸ’¡ Obserwuj jak AI pisze odpowiedÅº na Å¼ywo...")
        print("-" * 70)
        
        messages = [{"role": "user", "content": zadanie}]
        
        try:
            current_response = ""
            token_count = 0
            start_generation = time.time()
            
            async for chunk in token_streaming_ollama.chat_stream(selected_model, messages):
                if self.cancel_requested:
                    print("\n\nğŸ›‘ GENERACJA PRZERWANA PRZEZ OPERATORA")
                    print(f"â±ï¸ Przerwane po {time.time() - start_time:.1f} sekundach")
                    break
                
                if "error" in chunk:
                    print(f"\nâŒ BÅ‚Ä…d: {chunk['error']}")
                    if "timeout" in str(chunk['error']).lower():
                        print("ğŸ’¡ SprÃ³buj z prostszym zadaniem lub poczekaj na model")
                    break
                
                # WyciÄ…gnij token z odpowiedzi
                if "message" in chunk and "content" in chunk["message"]:
                    token = chunk["message"]["content"]
                    
                    if token:
                        # WyÅ›wietl token bez nowej linii (efekt live streaming)
                        print(token, end="", flush=True)
                        current_response += token
                        token_count += 1
                        
                        # MaÅ‚e opÃ³Åºnienie dla efektu wizualnego
                        await asyncio.sleep(0.02)  # 20ms dla lepszej czytelnoÅ›ci
                
                # SprawdÅº czy generacja zakoÅ„czona
                if chunk.get("done", False):
                    generation_time = time.time() - start_generation
                    total_time = time.time() - start_time
                    
                    print("\n\n" + "=" * 70)
                    print("ğŸ‰ GENERACJA ZAKOÅƒCZONA POMYÅšLNIE!")
                    print("=" * 70)
                    print(f"ğŸ“Š Statystyki:")
                    print(f"   â€¢ Wygenerowane tokeny: {token_count}")
                    print(f"   â€¢ CaÅ‚kowite znaki: {len(current_response)}")
                    print(f"   â€¢ UÅ¼yty model: {selected_model}")
                    print(f"   â€¢ Czas generacji: {generation_time:.1f}s")
                    print(f"   â€¢ Czas caÅ‚kowity: {total_time:.1f}s")
                    
                    if token_count > 0:
                        print(f"   â€¢ PrÄ™dkoÅ›Ä‡: {token_count/generation_time:.1f} tokenÃ³w/s")
                    
                    # PokaÅ¼ peÅ‚nÄ… odpowiedÅº w sformatowany sposÃ³b
                    print("\nğŸ“„ KOMPLETNA ODPOWIEDÅ¹:")
                    print("-" * 50)
                    print(current_response)
                    print("-" * 50)
                    
                    # Ocena jakoÅ›ci
                    if len(current_response) > 100:
                        print("âœ… JakoÅ›Ä‡: Obszerna odpowiedÅº")
                    elif len(current_response) > 50:
                        print("âœ… JakoÅ›Ä‡: Dobra odpowiedÅº")
                    else:
                        print("âš ï¸ JakoÅ›Ä‡: KrÃ³tka odpowiedÅº - moÅ¼e sprÃ³bowaÄ‡ ponownie")
                    
                    break
                    
        except Exception as e:
            print(f"\nâŒ BÅ‚Ä…d podczas streamingu: {e}")
            print("ğŸ’¡ SprawdÅº czy Ollama dziaÅ‚a: ollama ps")
        finally:
            self.current_task = None
            self.cancel_requested = False
    
    def _pokaz_status(self):
        """PokaÅ¼ status systemu po polsku"""
        print("\nğŸ“Š STATUS SYSTEMU:")
        print("-" * 30)
        status = "ZAJÄ˜TY" if self.current_task else "GOTOWY"
        print(f"ğŸ›ï¸ Dashboard: AKTYWNY")
        print(f"ğŸ¤– AI System: {status}")
        
        if self.current_task:
            print(f"ğŸ“‹ Obecne zadanie: {self.current_task[:40]}...")
        
        # SprawdÅº Ollama
        import subprocess
        try:
            result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0 and result.stdout.strip():
                print("âœ… Ollama: POÅÄ„CZONY")
                lines = result.stdout.strip().split('\n')[1:]  # Skip header
                if lines and lines[0].strip():
                    print(f"ğŸ¤– ZaÅ‚adowane modele: {len(lines)}")
            else:
                print("âš ï¸ Ollama: BRAK MODELI")
        except:
            print("âŒ Ollama: NIEDOSTÄ˜PNY")
    
    def _pokaz_pomoc(self):
        """PokaÅ¼ pomoc po polsku"""
        print("\nğŸ“š POMOC - DostÄ™pne Komendy:")
        print("-" * 40)
        print("ğŸ¯ [dowolny tekst] - Uruchom zadanie AI")
        print("ğŸ›‘ Ctrl+C - Przerwij obecnÄ… generacjÄ™")
        print("ğŸ“Š status/stan - SprawdÅº status systemu")
        print("â“ pomoc/help - PokaÅ¼ tÄ™ pomoc")
        print("ğŸ‘‹ koniec/quit - WyjdÅº z dashboard")
        print()
        print("ğŸ’¡ PrzykÅ‚ady zadaÅ„:")
        print('   â€¢ "napisz funkcjÄ™ hello world w pythonie"')
        print('   â€¢ "wyjaÅ›nij co to jest API"')
        print('   â€¢ "stwÃ³rz prosty HTML dla strony"')
        print('   â€¢ "napisz wiersz o kotach"')
        print()
        print("âš¡ WskazÃ³wki:")
        print("   â€¢ Pierwsze uruchomienie modelu moÅ¼e trwaÄ‡ 5-10 minut")
        print("   â€¢ MoÅ¼esz przerwaÄ‡ w kaÅ¼dej chwili przez Ctrl+C")
        print("   â€¢ Obserwuj jak AI pisze odpowiedÅº sÅ‚owo po sÅ‚owie")
        print("   â€¢ Proste zadania sÄ… szybsze (phi3:mini)")
        print("   â€¢ Zadania kodu uÅ¼ywajÄ… deepseek-coder (wolniejszy)")
    
    def _obsluz_przerwanie(self, signum, frame):
        """ObsÅ‚uÅ¼ Ctrl+C po polsku"""
        print(f"\nğŸš¨ PRZERWANIE - Operator zatrzymaÅ‚ operacjÄ™")
        self.cancel_requested = True
        print("ğŸ’¡ Wpisz 'koniec' aby wyjÅ›Ä‡ z dashboard")

async def main():
    """GÅ‚Ã³wna funkcja"""
    print("ğŸ›ï¸ Uruchamiam Polski Dashboard z Live Streaming...")
    dashboard = PolskiTokenDashboard()
    await dashboard.uruchom_dashboard()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Dashboard zamkniÄ™ty przez uÅ¼ytkownika")
    except Exception as e:
        print(f"\nâŒ Krytyczny bÅ‚Ä…d: {e}")
