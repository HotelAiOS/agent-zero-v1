
# AGENT ZERO V2.0 - INTELLIGENCE CORE
# M√≥zg kt√≥ry my≈õli, decyduje, pamiƒôta

intelligence_core = '''"""
Agent Zero V2.0 - Intelligence Core
M√≥zg systemu: my≈õli, decyduje, pamiƒôta, uczy siƒô

Dependencies: ollama, neo4j-driver, asyncio
"""

import asyncio
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging

# Neo4j dla pamiƒôci (opcjonalne - dzia≈Ça bez tego te≈º)
try:
    from neo4j import GraphDatabase
    NEO4J_AVAILABLE = True
except ImportError:
    NEO4J_AVAILABLE = False
    print("‚ö†Ô∏è  Neo4j not available - memory will be in-process only")

# Ollama dla AI (wymagane)
try:
    import httpx
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("‚ùå httpx required for Ollama - install: pip install httpx")


class TaskType(Enum):
    """Typy zada≈Ñ kt√≥re system rozpoznaje"""
    CODE_GENERATION = "code"
    BUSINESS_ANALYSIS = "business"
    ARCHITECTURE = "architecture"
    PROBLEM_SOLVING = "problem"
    PLANNING = "planning"
    OPTIMIZATION = "optimization"


class ModelChoice(Enum):
    """Dostƒôpne modele Ollama"""
    QUICK = "llama3.1:8b"        # Szybki, tani - proste zadania
    SMART = "codellama:13b"      # Balans - wiƒôkszo≈õƒá zada≈Ñ
    EXPERT = "qwen2.5-coder:32b" # Wolny, drogi - trudne zadania
    BUSINESS = "llama3.1:70b"    # Najwiƒôkszy - strategia (je≈õli dostƒôpny)


@dataclass
class ThinkingResult:
    """Wynik my≈õlenia systemu"""
    task_type: TaskType
    complexity: int  # 1-10
    selected_model: ModelChoice
    reasoning: str
    confidence: float
    estimated_tokens: int
    expected_quality: float


@dataclass
class ExecutionResult:
    """Wynik wykonania zadania"""
    task_id: str
    model_used: str
    result_text: str
    actual_tokens: int
    execution_time: float
    quality_score: float
    learned_patterns: List[str]


class AgentZeroIntelligence:
    """
    üß† Centralny M√≥zg Agent Zero V2.0
    
    Rola: MY≈öLI o zadaniu, DECYDUJE jak je wykonaƒá, PAMIƒòTA co dzia≈Ça
    """
    
    def __init__(self, 
                 ollama_url: str = "http://localhost:11434",
                 neo4j_uri: Optional[str] = None,
                 neo4j_user: Optional[str] = None,
                 neo4j_password: Optional[str] = None):
        
        self.logger = logging.getLogger(__name__)
        self.ollama_url = ollama_url
        
        # Pamiƒôƒá w procesie (zawsze dzia≈Ça)
        self.execution_memory = []
        self.pattern_library = {}
        
        # Pamiƒôƒá Neo4j (opcjonalna)
        self.neo4j_driver = None
        if NEO4J_AVAILABLE and neo4j_uri:
            try:
                self.neo4j_driver = GraphDatabase.driver(
                    neo4j_uri,
                    auth=(neo4j_user, neo4j_password)
                )
                self.logger.info("‚úÖ Neo4j memory connected")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è  Neo4j connection failed: {e}")
        
        # Model performance tracking
        self.model_stats = {model.value: {"uses": 0, "avg_quality": 0.0} 
                           for model in ModelChoice}
        
        self.logger.info("üß† Intelligence Core initialized")
    
    async def think_and_execute(self, user_request: str, 
                               context: Optional[Dict] = None) -> ExecutionResult:
        """
        G≈Å√ìWNA FUNKCJA: My≈õli o zadaniu i wykonuje je inteligentnie
        
        1. MY≈öLI: Co to za zadanie? Jaka z≈Ço≈ºono≈õƒá?
        2. DECYDUJE: Kt√≥ry model bƒôdzie najlepszy?
        3. WYKONUJE: Z optymalnym modelem
        4. UCZY SIƒò: Zapisuje co zadzia≈Ça≈Ço
        """
        context = context or {}
        task_id = self._generate_task_id(user_request)
        
        self.logger.info(f"üéØ Thinking about task: {task_id}")
        
        # STAGE 1: MY≈öLENIE - Analiza zadania
        thinking = await self._think_about_task(user_request, context)
        self.logger.info(f"üí° Decision: {thinking.selected_model.value} "
                        f"(confidence: {thinking.confidence:.0%})")
        
        # STAGE 2: WYKONANIE - Z najlepszym modelem
        result = await self._execute_with_model(
            task_id, user_request, thinking, context
        )
        
        # STAGE 3: UCZENIE - Zapisz do≈õwiadczenie
        await self._learn_from_execution(thinking, result)
        
        return result
    
    async def _think_about_task(self, request: str, 
                               context: Dict) -> ThinkingResult:
        """
        Inteligentna analiza zadania - to jest MY≈öLENIE
        """
        # Rozpoznaj typ zadania
        task_type = self._classify_task_type(request)
        
        # Oce≈Ñ z≈Ço≈ºono≈õƒá (1-10)
        complexity = self._assess_complexity(request, task_type)
        
        # Sprawd≈∫ czy mamy podobne do≈õwiadczenia
        similar_tasks = self._find_similar_experiences(request, task_type)
        
        # Wybierz model bazujƒÖc na:
        # 1. Typ zadania
        # 2. Z≈Ço≈ºono≈õƒá
        # 3. Poprzednie do≈õwiadczenia
        # 4. Dostƒôpno≈õƒá modelu
        selected_model = await self._select_best_model(
            task_type, complexity, similar_tasks, context
        )
        
        # Oblicz confidence i predykcje
        confidence = self._calculate_confidence(
            task_type, complexity, similar_tasks
        )
        
        estimated_tokens = self._estimate_tokens(request, complexity)
        expected_quality = self._predict_quality(
            selected_model, task_type, complexity
        )
        
        reasoning = self._generate_reasoning(
            task_type, complexity, selected_model, similar_tasks
        )
        
        return ThinkingResult(
            task_type=task_type,
            complexity=complexity,
            selected_model=selected_model,
            reasoning=reasoning,
            confidence=confidence,
            estimated_tokens=estimated_tokens,
            expected_quality=expected_quality
        )
    
    def _classify_task_type(self, request: str) -> TaskType:
        """Rozpoznaj typ zadania na podstawie tre≈õci"""
        request_lower = request.lower()
        
        # Wzorce dla ka≈ºdego typu
        patterns = {
            TaskType.CODE_GENERATION: [
                "kod", "code", "funkcj", "class", "api", "implement",
                "napisz", "stw√≥rz", "zaimplementuj"
            ],
            TaskType.BUSINESS_ANALYSIS: [
                "biznes", "business", "analiz", "wymagania", "requirements",
                "stakeholder", "roi", "strategia"
            ],
            TaskType.ARCHITECTURE: [
                "architektura", "architecture", "design", "system",
                "skalowalnos", "pattern", "struktura"
            ],
            TaskType.PROBLEM_SOLVING: [
                "problem", "b≈ÇƒÖd", "error", "debug", "napraw", "fix",
                "rozwiƒÖ≈º", "troubleshoot"
            ],
            TaskType.PLANNING: [
                "plan", "harmonogram", "timeline", "roadmap",
                "zaplanuj", "schedule"
            ],
            TaskType.OPTIMIZATION: [
                "optymalizuj", "optimize", "improve", "wydajnos",
                "performance", "przyspiesz"
            ]
        }
        
        # Zlicz trafienia
        scores = {}
        for task_type, keywords in patterns.items():
            score = sum(1 for keyword in keywords if keyword in request_lower)
            if score > 0:
                scores[task_type] = score
        
        # Zwr√≥ƒá najlepszy match lub default
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]
        
        return TaskType.PROBLEM_SOLVING  # Default
    
    def _assess_complexity(self, request: str, task_type: TaskType) -> int:
        """Oce≈Ñ z≈Ço≈ºono≈õƒá zadania (1-10)"""
        complexity = 5  # Base
        
        # D≈Çugo≈õƒá requestu
        word_count = len(request.split())
        if word_count > 100:
            complexity += 2
        elif word_count > 50:
            complexity += 1
        
        # S≈Çowa kluczowe z≈Ço≈ºono≈õci
        high_complexity = [
            "complex", "advanced", "enterprise", "scalable",
            "distributed", "real-time", "machine learning"
        ]
        medium_complexity = [
            "integrate", "optimize", "refactor", "design"
        ]
        
        request_lower = request.lower()
        
        if any(word in request_lower for word in high_complexity):
            complexity += 3
        elif any(word in request_lower for word in medium_complexity):
            complexity += 1
        
        # Typ zadania wp≈Çywa na bazowƒÖ z≈Ço≈ºono≈õƒá
        type_complexity = {
            TaskType.CODE_GENERATION: 0,
            TaskType.BUSINESS_ANALYSIS: 1,
            TaskType.ARCHITECTURE: 2,
            TaskType.PROBLEM_SOLVING: 1,
            TaskType.PLANNING: 1,
            TaskType.OPTIMIZATION: 2
        }
        
        complexity += type_complexity.get(task_type, 0)
        
        return min(max(complexity, 1), 10)  # Clamp 1-10
    
    def _find_similar_experiences(self, request: str, 
                                 task_type: TaskType) -> List[Dict]:
        """Znajd≈∫ podobne zadania z przesz≈Ço≈õci"""
        similar = []
        
        for memory in self.execution_memory:
            # Sprawd≈∫ czy ten sam typ
            if memory.get('task_type') != task_type.value:
                continue
            
            # Sprawd≈∫ podobie≈Ñstwo tekstowe (prosty matching)
            similarity = self._calculate_similarity(
                request, memory.get('request', '')
            )
            
            if similarity > 0.3:  # 30% podobie≈Ñstwa
                similar.append({
                    'memory': memory,
                    'similarity': similarity
                })
        
        # Sortuj po podobie≈Ñstwie
        similar.sort(key=lambda x: x['similarity'], reverse=True)
        return similar[:3]  # Top 3
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Prosta miara podobie≈Ñstwa tekst√≥w"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    async def _select_best_model(self, task_type: TaskType, 
                                complexity: int,
                                similar_tasks: List[Dict],
                                context: Dict) -> ModelChoice:
        """
        KLUCZOWA DECYZJA: Wyb√≥r najlepszego modelu
        
        Bazuje na:
        - Typ zadania
        - Z≈Ço≈ºono≈õƒá
        - Poprzednie do≈õwiadczenia
        - Priorytet (jako≈õƒá vs szybko≈õƒá vs koszt)
        """
        # Priorytet z contextu
        quality_priority = context.get('quality_priority', 0.7)
        speed_priority = context.get('speed_priority', 0.2)
        
        # Sprawd≈∫ dostƒôpne modele
        available_models = await self._check_available_models()
        
        # Regu≈Çy decyzyjne
        
        # 1. Bardzo proste zadania ‚Üí QUICK
        if complexity <= 3:
            if ModelChoice.QUICK.value in available_models:
                return ModelChoice.QUICK
        
        # 2. Kod o ≈õredniej z≈Ço≈ºono≈õci ‚Üí SMART (CodeLlama)
        if task_type == TaskType.CODE_GENERATION and complexity <= 7:
            if ModelChoice.SMART.value in available_models:
                return ModelChoice.SMART
        
        # 3. Trudne zadania architektoniczne ‚Üí EXPERT
        if task_type == TaskType.ARCHITECTURE and complexity >= 6:
            if ModelChoice.EXPERT.value in available_models:
                return ModelChoice.EXPERT
        
        # 4. Biznes i strategia ‚Üí najlepszy dostƒôpny
        if task_type == TaskType.BUSINESS_ANALYSIS:
            if ModelChoice.BUSINESS.value in available_models and complexity >= 8:
                return ModelChoice.BUSINESS
            elif ModelChoice.EXPERT.value in available_models:
                return ModelChoice.EXPERT
        
        # 5. Uczenie siƒô z przesz≈Ço≈õci
        if similar_tasks:
            # Sprawd≈∫ kt√≥ry model dzia≈Ça≈Ç najlepiej
            best_past_model = self._find_best_performing_model(similar_tasks)
            if best_past_model and best_past_model in available_models:
                return ModelChoice(best_past_model)
        
        # 6. Priorytet jako≈õci ‚Üí wiƒôkszy model
        if quality_priority > 0.8:
            if ModelChoice.EXPERT.value in available_models:
                return ModelChoice.EXPERT
        
        # 7. Priorytet szybko≈õci ‚Üí mniejszy model
        if speed_priority > 0.5:
            if ModelChoice.QUICK.value in available_models:
                return ModelChoice.QUICK
        
        # 8. Default: SMART (balans)
        if ModelChoice.SMART.value in available_models:
            return ModelChoice.SMART
        
        # 9. Fallback: pierwszy dostƒôpny
        for model in ModelChoice:
            if model.value in available_models:
                return model
        
        # 10. Ostateczny fallback
        return ModelChoice.QUICK
    
    async def _check_available_models(self) -> List[str]:
        """Sprawd≈∫ kt√≥re modele sƒÖ dostƒôpne w Ollama"""
        if not OLLAMA_AVAILABLE:
            return [ModelChoice.QUICK.value]  # Fallback
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    return [model['name'] for model in data.get('models', [])]
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  Could not check Ollama models: {e}")
        
        return [ModelChoice.QUICK.value]  # Fallback
    
    def _find_best_performing_model(self, similar_tasks: List[Dict]) -> Optional[str]:
        """Znajd≈∫ model kt√≥ry najlepiej dzia≈Ça≈Ç dla podobnych zada≈Ñ"""
        model_scores = {}
        
        for task_info in similar_tasks:
            memory = task_info['memory']
            model = memory.get('model_used')
            quality = memory.get('quality_score', 0)
            
            if model:
                if model not in model_scores:
                    model_scores[model] = []
                model_scores[model].append(quality)
        
        if not model_scores:
            return None
        
        # Oblicz ≈õredniƒÖ jako≈õƒá dla ka≈ºdego modelu
        avg_scores = {
            model: sum(scores) / len(scores)
            for model, scores in model_scores.items()
        }
        
        # Zwr√≥ƒá najlepszy
        best_model = max(avg_scores.items(), key=lambda x: x[1])[0]
        return best_model
    
    def _calculate_confidence(self, task_type: TaskType, 
                            complexity: int,
                            similar_tasks: List[Dict]) -> float:
        """Oblicz confidence w decyzji"""
        confidence = 0.5  # Base
        
        # Mamy podobne zadania ‚Üí wy≈ºsza confidence
        if similar_tasks:
            confidence += 0.2 * min(len(similar_tasks) / 3, 1.0)
        
        # Niska z≈Ço≈ºono≈õƒá ‚Üí wy≈ºsza confidence
        if complexity <= 5:
            confidence += 0.2
        
        # Typ zadania kt√≥ry dobrze znamy
        if task_type in [TaskType.CODE_GENERATION, TaskType.PROBLEM_SOLVING]:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _estimate_tokens(self, request: str, complexity: int) -> int:
        """Oszacuj ile token√≥w u≈ºyje model"""
        base_tokens = len(request.split()) * 4  # Rough estimate
        complexity_multiplier = 1 + (complexity / 10)
        return int(base_tokens * complexity_multiplier)
    
    def _predict_quality(self, model: ModelChoice, 
                        task_type: TaskType, complexity: int) -> float:
        """Przewiduj jako≈õƒá wyniku"""
        # Bazowa jako≈õƒá modelu
        model_quality = {
            ModelChoice.QUICK: 0.7,
            ModelChoice.SMART: 0.85,
            ModelChoice.EXPERT: 0.95,
            ModelChoice.BUSINESS: 0.9
        }
        
        base = model_quality.get(model, 0.7)
        
        # Dopasowanie model-zadanie
        if model == ModelChoice.SMART and task_type == TaskType.CODE_GENERATION:
            base += 0.05  # CodeLlama jest ≈õwietny w kodzie
        
        # Z≈Ço≈ºono≈õƒá vs moc modelu
        model_size = list(ModelChoice).index(model)
        if complexity > 7 and model_size < 2:  # Trudne zadanie, s≈Çaby model
            base -= 0.15
        
        return min(base, 1.0)
    
    def _generate_reasoning(self, task_type: TaskType, complexity: int,
                          model: ModelChoice, similar_tasks: List[Dict]) -> str:
        """Wygeneruj uzasadnienie decyzji"""
        reasons = []
        
        reasons.append(f"Task type: {task_type.value}")
        reasons.append(f"Complexity: {complexity}/10")
        reasons.append(f"Selected: {model.value}")
        
        if similar_tasks:
            reasons.append(f"Based on {len(similar_tasks)} similar past tasks")
        
        if complexity <= 3:
            reasons.append("Low complexity ‚Üí fast model sufficient")
        elif complexity >= 8:
            reasons.append("High complexity ‚Üí expert model needed")
        
        return " | ".join(reasons)
    
    async def _execute_with_model(self, task_id: str, request: str,
                                 thinking: ThinkingResult,
                                 context: Dict) -> ExecutionResult:
        """Wykonaj zadanie z wybranym modelem"""
        self.logger.info(f"üöÄ Executing with {thinking.selected_model.value}")
        
        start_time = datetime.now()
        
        # Przygotuj prompt
        optimized_prompt = self._optimize_prompt(request, thinking)
        
        # Wywo≈Çaj Ollama
        result_text = await self._call_ollama(
            thinking.selected_model.value,
            optimized_prompt
        )
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # Oce≈Ñ jako≈õƒá wyniku
        quality_score = self._assess_quality(result_text, thinking)
        
        # Wyodrƒôbnij wzorce do uczenia
        learned_patterns = self._extract_patterns(
            request, result_text, thinking, quality_score
        )
        
        return ExecutionResult(
            task_id=task_id,
            model_used=thinking.selected_model.value,
            result_text=result_text,
            actual_tokens=len(result_text.split()) * 4,
            execution_time=execution_time,
            quality_score=quality_score,
            learned_patterns=learned_patterns
        )
    
    def _optimize_prompt(self, request: str, thinking: ThinkingResult) -> str:
        """Optymalizuj prompt dla konkretnego modelu"""
        prompt_parts = [request]
        
        # Dodaj kontekst bazujƒÖc na typie zadania
        if thinking.task_type == TaskType.CODE_GENERATION:
            prompt_parts.append("\nProvide clean, production-ready code with comments.")
        elif thinking.task_type == TaskType.ARCHITECTURE:
            prompt_parts.append("\nFocus on scalability, maintainability, and best practices.")
        elif thinking.task_type == TaskType.BUSINESS_ANALYSIS:
            prompt_parts.append("\nProvide clear business value and ROI considerations.")
        
        # Dodaj instrukcje jako≈õci dla trudnych zada≈Ñ
        if thinking.complexity >= 7:
            prompt_parts.append("\nThis is a complex task - take time to think through edge cases.")
        
        return "\n".join(prompt_parts)
    
    async def _call_ollama(self, model: str, prompt: str) -> str:
        """Wywo≈Çaj Ollama API"""
        if not OLLAMA_AVAILABLE:
            return f"[DEMO MODE] Response for: {prompt[:50]}..."
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get('response', '')
                else:
                    self.logger.error(f"Ollama error: {response.status_code}")
                    return f"Error calling Ollama: {response.status_code}"
                    
        except Exception as e:
            self.logger.error(f"Ollama call failed: {e}")
            return f"Error: {str(e)}"
    
    def _assess_quality(self, result: str, thinking: ThinkingResult) -> float:
        """Oce≈Ñ jako≈õƒá wyniku"""
        quality = 0.5  # Base
        
        # D≈Çugo≈õƒá odpowiedzi
        if len(result) > 100:
            quality += 0.2
        
        # Struktura (ma paragrafy/sekcje)
        if result.count('\n') >= 2:
            quality += 0.1
        
        # Specyficzne dla typu zadania
        if thinking.task_type == TaskType.CODE_GENERATION:
            if 'def ' in result or 'class ' in result or 'function' in result:
                quality += 0.2
        
        return min(quality, 1.0)
    
    def _extract_patterns(self, request: str, result: str,
                         thinking: ThinkingResult, quality: float) -> List[str]:
        """Wyodrƒôbnij wzorce do uczenia siƒô"""
        patterns = []
        
        if quality > 0.8:
            patterns.append(f"GOOD: {thinking.task_type.value} with {thinking.selected_model.value}")
        
        if quality < 0.5:
            patterns.append(f"POOR: {thinking.task_type.value} with {thinking.selected_model.value}")
        
        return patterns
    
    async def _learn_from_execution(self, thinking: ThinkingResult,
                                   result: ExecutionResult):
        """Zapisz do≈õwiadczenie do pamiƒôci"""
        # Pamiƒôƒá w procesie
        memory_entry = {
            'task_type': thinking.task_type.value,
            'complexity': thinking.complexity,
            'model_used': result.model_used,
            'quality_score': result.quality_score,
            'execution_time': result.execution_time,
            'timestamp': datetime.now().isoformat(),
            'learned_patterns': result.learned_patterns
        }
        
        self.execution_memory.append(memory_entry)
        
        # Update model stats
        model = result.model_used
        if model in self.model_stats:
            stats = self.model_stats[model]
            old_avg = stats['avg_quality']
            old_count = stats['uses']
            
            stats['uses'] = old_count + 1
            stats['avg_quality'] = (old_avg * old_count + result.quality_score) / stats['uses']
        
        # Neo4j (je≈õli dostƒôpny)
        if self.neo4j_driver:
            await self._store_in_neo4j(memory_entry)
        
        self.logger.info(f"üìö Learned from execution (quality: {result.quality_score:.0%})")
    
    async def _store_in_neo4j(self, memory: Dict):
        """Zapisz do Neo4j (persistent memory)"""
        try:
            with self.neo4j_driver.session() as session:
                session.run("""
                    CREATE (e:Execution {
                        task_type: $task_type,
                        complexity: $complexity,
                        model_used: $model_used,
                        quality_score: $quality_score,
                        timestamp: $timestamp
                    })
                """, **memory)
        except Exception as e:
            self.logger.warning(f"Neo4j store failed: {e}")
    
    def _generate_task_id(self, request: str) -> str:
        """Generuj unikalny ID zadania"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        hash_part = hashlib.md5(request.encode()).hexdigest()[:8]
        return f"task_{timestamp}_{hash_part}"
    
    def get_statistics(self) -> Dict[str, Any]:
        """Pobierz statystyki systemu"""
        return {
            'total_executions': len(self.execution_memory),
            'model_stats': self.model_stats,
            'avg_quality': sum(m['quality_score'] for m in self.execution_memory) / len(self.execution_memory) if self.execution_memory else 0,
            'memory_available': NEO4J_AVAILABLE and self.neo4j_driver is not None
        }
    
    def close(self):
        """Zamknij po≈ÇƒÖczenia"""
        if self.neo4j_driver:
            self.neo4j_driver.close()


# ============================================================================
# DEMO / TEST
# ============================================================================

async def demo_intelligence_core():
    """Demonstracja Intelligence Core"""
    print("üß† Agent Zero V2.0 - Intelligence Core Demo")
    print("=" * 60)
    
    # Initialize
    brain = AgentZeroIntelligence()
    
    # Test Case 1: Proste zadanie kodowe
    print("\nüìù Test 1: Simple Code Generation")
    result1 = await brain.think_and_execute(
        "Napisz funkcjƒô Python do obliczania liczb Fibonacciego"
    )
    print(f"‚úÖ Model used: {result1.model_used}")
    print(f"üìä Quality: {result1.quality_score:.0%}")
    print(f"‚è±Ô∏è  Time: {result1.execution_time:.2f}s")
    
    # Test Case 2: Trudne zadanie architektoniczne
    print("\nüìù Test 2: Complex Architecture")
    result2 = await brain.think_and_execute(
        "Zaprojektuj skalowalnƒÖ architekturƒô mikrous≈Çug dla systemu e-commerce obs≈ÇugujƒÖcego 1M u≈ºytkownik√≥w",
        context={'quality_priority': 0.9}
    )
    print(f"‚úÖ Model used: {result2.model_used}")
    print(f"üìä Quality: {result2.quality_score:.0%}")
    print(f"‚è±Ô∏è  Time: {result2.execution_time:.2f}s")
    
    # Statistics
    print("\nüìà System Statistics:")
    stats = brain.get_statistics()
    print(f"Total executions: {stats['total_executions']}")
    print(f"Average quality: {stats['avg_quality']:.0%}")
    print(f"Model performance:")
    for model, model_stats in stats['model_stats'].items():
        if model_stats['uses'] > 0:
            print(f"  {model}: {model_stats['uses']} uses, avg quality {model_stats['avg_quality']:.0%}")
    
    brain.close()
    print("\nüéâ Intelligence Core working perfectly!")


if __name__ == "__main__":
    import asyncio
    asyncio.run(demo_intelligence_core())
'''

# Zapisz plik
with open('intelligence_core.py', 'w', encoding='utf-8') as f:
    f.write(intelligence_core)

print("‚úÖ PLIK 1/3 GOTOWY: intelligence_core.py")
print("\nüìã CO ROBI TEN PLIK:")
print("  üß† MY≈öLI o zadaniu (typ, z≈Ço≈ºono≈õƒá)")
print("  üéØ DECYDUJE kt√≥ry model Ollama u≈ºyƒá")
print("  üöÄ WYKONUJE z optymalnym modelem")
print("  üìö UCZY SIƒò z ka≈ºdego wykonania")
print("  üíæ PAMIƒòTA co dzia≈Ça (in-memory + Neo4j opcjonalnie)")
print("\n‚ú® KLUCZOWE FUNKCJE:")
print("  ‚Ä¢ think_and_execute() - g≈Ç√≥wna funkcja")
print("  ‚Ä¢ Automatyczny wyb√≥r modelu (llama3.1:8b / codellama:13b / qwen2.5-coder:32b)")
print("  ‚Ä¢ Uczenie siƒô z do≈õwiadcze≈Ñ")
print("  ‚Ä¢ Statystyki wydajno≈õci modeli")
print("\n‚ö° GOTOWE DO U≈ªYCIA - dzia≈Ça standalone!")
